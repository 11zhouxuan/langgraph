{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STORM\n",
    "\n",
    "[STORM](https://arxiv.org/abs/2402.14207) is a research assistant by Shao, et. al that extends the idea of \"outline-driven RAG\" for richer article generation.\n",
    "\n",
    "It is tasked with generating Wikipedia-like ariticles on a user-provided topic.  It has  a few main stages:\n",
    "\n",
    "1. Survey related subjects\n",
    "2. Identify perspectives\n",
    "3. \"Expert Interviews\" (between the writer and an agent role-playing as a perspective)\n",
    "4. Refine article \n",
    "\n",
    "The expert interviews stage ocurrs between the article writer and each role-playing agent and itself is a loop, where the \"expert\" is able to query external knowledge and respond to pointed questions.\n",
    "\n",
    "Couple hyperparameters to restrict the infinite research breadth:\n",
    "\n",
    "N: Number of perspectives to survey / use (2->3)\n",
    "M: Max number of conversation turns in step (3)\n",
    "\n",
    "The paper uses DSPY and few-shot examples to adapt but we'll just use functioncalling here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_community langchain_openai langgraph wikipedia tavily-python scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "fast_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "long_context_llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/code/lc/community/langgraph-engineer/.venv/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:86: LangChainBetaWarning: The function `with_structured_output` is in beta. It is actively being worked on, so the API may change.\n",
      "  warn_beta(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "direct_gen_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.\",\n",
    "        ),\n",
    "        (\"user\", \"{topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Subsection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    description: str = Field(..., title=\"Content of the subsection\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()\n",
    "\n",
    "\n",
    "class Section(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    description: str = Field(..., title=\"Content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            f\"### {subsection.subsection_title}\\n\\n{subsection.description}\"\n",
    "            for subsection in self.subsections or []\n",
    "        )\n",
    "        return f\"## {self.section_title}\\n\\n{self.description}\\n\\n{subsections}\".strip()\n",
    "\n",
    "\n",
    "class Outline(BaseModel):\n",
    "    page_title: str = Field(..., title=\"Title of the Wikipedia page\")\n",
    "    sections: List[Section] = Field(\n",
    "        default_factory=list,\n",
    "        title=\"Titles and descriptions for each section of the Wikipedia page.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n",
    "        return f\"# {self.page_title}\\n\\n{sections}\".strip()\n",
    "\n",
    "\n",
    "generate_outline_direct = direct_gen_outline_prompt | fast_llm.with_structured_output(\n",
    "    Outline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of million-plus token context window language models on RAG\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Overview of million-plus token context window language models and the RAG (Retrieval-Augmented Generation) architecture.\n",
      "\n",
      "## Benefits of Million-Plus Token Context Window Language Models\n",
      "\n",
      "Discuss the advantages of using million-plus token context window language models in natural language processing tasks.\n",
      "\n",
      "## Challenges of Million-Plus Token Context Window Language Models\n",
      "\n",
      "Explore the limitations and obstacles associated with million-plus token context window language models.\n",
      "\n",
      "## Integration of Million-Plus Token Models with RAG\n",
      "\n",
      "Examine how million-plus token context window language models can be integrated with the RAG architecture for improved performance.\n",
      "\n",
      "## Applications of RAG with Million-Plus Token Models\n",
      "\n",
      "Highlight the potential applications and use cases of combining RAG with million-plus token context window language models.\n"
     ]
    }
   ],
   "source": [
    "example_topic = \"Impact of million-plus token context window language models on RAG\"\n",
    "\n",
    "initial_outline = generate_outline_direct.invoke({\"topic\": example_topic})\n",
    "\n",
    "print(initial_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand Topics\n",
    "\n",
    "While language models do store some Wikipedia-like knowledge in their parameters, you will get better results by incorporating relevant and recent information using a search engine.\n",
    "\n",
    "We will start our search by generating a list of related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I'm writing a Wikipedia page for a topic mentioned below. Please identify and recommend some Wikipedia pages on closely related subjects. I'm looking for examples that provide insights into interesting aspects commonly associated with this topic, or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics.\n",
    "\n",
    "Please list the as many subjects and urls as you can.\n",
    "\n",
    "Topic of interest: {topic}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "class RelatedSubjects(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "        description=\"Comprehensive list of related subjects as background research.\",\n",
    "    )\n",
    "\n",
    "\n",
    "expand_chain = gen_related_topics_prompt | fast_llm.with_structured_output(\n",
    "    RelatedSubjects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RelatedSubjects(topics=['million-plus token context window language models', 'Retriever-Reader-Generator (RAG) model', 'Impact of RAG on language understanding'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_subjects = await expand_chain.ainvoke({\"topic\": example_topic})\n",
    "related_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor(BaseModel):\n",
    "    affiliation: str = Field(\n",
    "        description=\"Primary affiliation of the editor.\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the editor.\",\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the editor in the context of the topic.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the editor's focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor] = Field(\n",
    "        description=\"Comprehensive list of editors with their roles and affiliations.\",\n",
    "        # Add a pydantic validation/restriction to be at most M editors\n",
    "    )\n",
    "\n",
    "\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You need to select a diverse (and distinct) group of Wikipedia editors who will work together to create a comprehensive article on the topic. Each of them represents a different perspective, role, or affiliation related to this topic.\\\n",
    "    You can use other Wikipedia pages of related topics for inspiration. For each editor, add a description of what they will focus on.\n",
    "\n",
    "    Wiki page outlines of related topics for inspiration:\n",
    "    {examples}\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Topic of interest: {topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt | ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ").with_structured_output(Perspectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import RunnableLambda, chain as as_runnable\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "\n",
    "def format_doc(doc, max_length=1000):\n",
    "    related = \"- \".join(doc.metadata[\"categories\"])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\n\\nRelated\\n{related}\"[\n",
    "        :max_length\n",
    "    ]\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(format_doc(doc) for doc in docs)\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topic: str):\n",
    "    related_subjects = await expand_chain.ainvoke({\"topic\": topic})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(\n",
    "        related_subjects.topics, return_exceptions=True\n",
    "    )\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\"examples\": formatted, \"topic\": topic})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/wfh/code/lc/community/langgraph-engineer/.venv/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /Users/wfh/code/lc/community/langgraph-engineer/.venv/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    }
   ],
   "source": [
    "perspectives = await survey_subjects.ainvoke(example_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affiliation': 'Research Institution',\n",
       "   'name': 'Dr. Researcher',\n",
       "   'role': 'Researcher',\n",
       "   'description': 'Dr. Researcher will focus on analyzing the impact of million-plus token context window language models on the RAG (Retrieval-Augmented Generation) framework, specifically looking at the efficiency, effectiveness, and potential challenges that arise from integrating such large language models into the RAG framework.'},\n",
       "  {'affiliation': 'Tech Company',\n",
       "   'name': 'AI Engineer',\n",
       "   'role': 'AI Engineer',\n",
       "   'description': 'AI Engineer will provide insights into the technical aspects of implementing million-plus token context window language models within the RAG framework. They will focus on the practical challenges, optimizations, and enhancements needed to leverage these models effectively in the RAG framework.'},\n",
       "  {'affiliation': 'Academic Institution',\n",
       "   'name': 'Prof. Linguist',\n",
       "   'role': 'Linguist',\n",
       "   'description': 'Prof. Linguist will examine the linguistic implications of using million-plus token context window language models in the RAG framework. They will explore how such large models affect language generation, coherence, and understanding within the RAG context.'},\n",
       "  {'affiliation': 'Industry',\n",
       "   'name': 'Content Creator',\n",
       "   'role': 'Content Creator',\n",
       "   'description': 'Content Creator will provide a creative perspective on the impact of million-plus token context window language models on the RAG framework. They will focus on storytelling, narrative quality, and the potential for generating engaging content using these large language models.'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspectives.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Dialog\n",
    "\n",
    "Now the true fun begins, the wikipedia writer will \"talk\" with expert agents primed to role-play using the perspectives presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing import Annotated, Sequence\n",
    "\n",
    "\n",
    "def add_messages(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left + right\n",
    "\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # Can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an experienced Wikipedia writer and want to edit a specific page. \\\n",
    "Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \\\n",
    "Now, you are chatting with an expert to get information. Ask good questions to get more useful information.\n",
    "\n",
    "When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation.\\\n",
    "Please only ask one question at a time and don't ask what you have asked before.\\\n",
    "Your questions should be related to the topic you want to write.\n",
    "Be comprehensive and curious, gaining as much unique insight from the expert as possible.\\\n",
    "\n",
    "Stay true to your specific perspective:\n",
    "\n",
    "{persona}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name: str):\n",
    "    ai_message.name = name\n",
    "    return ai_message\n",
    "\n",
    "\n",
    "def swap_roles(state: InterviewState, name: str):\n",
    "    converted = []\n",
    "    for message in state[\"messages\"]:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.dict(exclude={\"type\"}))\n",
    "        converted.append(message)\n",
    "    return {\"messages\": converted}\n",
    "\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state: InterviewState):\n",
    "    editor = state[\"editor\"]\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, that's correct. I am researching the impact of million-plus token context window language models on the RAG (Retrieval-Augmented Generation) framework. I am particularly interested in understanding how these large language models affect the efficiency, effectiveness, and any potential challenges that may arise when integrating them into the RAG framework. Is there any specific aspect of this topic that you would like to know more about?\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(f\"So you said you were writing an article on {example_topic}?\")\n",
    "]\n",
    "question = await generate_question.ainvoke(\n",
    "    {\n",
    "        \"editor\": perspectives.editors[0],\n",
    "        \"messages\": messages,\n",
    "    }\n",
    ")\n",
    "\n",
    "question[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        description=\"Comprehensive list of search engine queries to answer the user's questions.\",\n",
    "    )\n",
    "\n",
    "\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful research assistant. Query the search engine to answer the user's questions.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "gen_queries_chain = gen_queries_prompt | ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ").with_structured_output(Queries, include_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Impact of million-plus token context window language models on RAG framework efficiency',\n",
       " 'Effectiveness of large language models in RAG framework',\n",
       " 'Challenges of integrating million-plus token context window language models into RAG framework']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = await gen_queries_chain.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "queries[\"parsed\"].queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithCitations(BaseModel):\n",
    "    answer: str = Field(\n",
    "        description=\"Comprehensive answer to the user's question with citations.\",\n",
    "    )\n",
    "    cited_urls: List[str] = Field(\n",
    "        description=\"List of urls cited in the answer.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"{self.answer}\\n\\nCitations:\\n\\n\" + \"\\n\".join(\n",
    "            f\"[{i+1}]: {url}\" for i, url in enumerate(self.cited_urls)\n",
    "        )\n",
    "\n",
    "\n",
    "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants\\\n",
    " to write a Wikipedia page on the topic you know. You have gathered the related information and will now use the information to form a response.\n",
    "\n",
    "Make your response as informative as possible and make sure every sentence is supported by the gathered information.\n",
    "Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLS after your response.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_answer_chain = gen_answer_prompt | fast_llm.with_structured_output(\n",
    "    AnswerWithCitations, include_raw=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference Store\n",
    "\n",
    "The research process uncovers a large number of reference documents that we may want to query during the final article-writing process.\n",
    "Here, we will createa multi-vector retriever and store all the searched documents inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "\n",
    "search_engine = DuckDuckGoSearchAPIWrapper()\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "# TODO: remove when i get my api limit bumped\n",
    "@tool\n",
    "async def search_engine(query: str):\n",
    "    \"\"\"Search engine to the internet.\"\"\"\n",
    "    results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)\n",
    "    return [{\"content\": r[\"body\"], \"url\": r[\"href\"]} for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "import json\n",
    "\n",
    "# search_engine = TavilySearchResults(max_results=4)\n",
    "\n",
    "\n",
    "async def gen_answer(\n",
    "    state: InterviewState,\n",
    "    config: RunnableConfig | None = None,\n",
    "    name: str = \"Subject Matter Expert\",\n",
    "    max_str_len: int = 15000,\n",
    "):\n",
    "    swapped_state = swap_roles(state, name)  # Convert all other AI messages\n",
    "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "    query_results = await search_engine.abatch(\n",
    "        queries[\"parsed\"].queries, config, return_exceptions=True\n",
    "    )\n",
    "    successful_results = [\n",
    "        res for res in query_results if not isinstance(res, Exception)\n",
    "    ]\n",
    "    all_query_results = {\n",
    "        res[\"url\"]: res[\"content\"] for results in successful_results for res in results\n",
    "    }\n",
    "    # We could be more precise about handling max token length if we wanted to here\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    ai_message: AIMessage = queries[\"raw\"]\n",
    "    tool_call = queries[\"raw\"].additional_kwargs[\"tool_calls\"][0]\n",
    "    tool_id = tool_call[\"id\"]\n",
    "    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "    swapped_state[\"messages\"].extend([ai_message, tool_message])\n",
    "    # Only update the shared state with the final answer to avoid\n",
    "    # polluting the dialogue history with intermediate messages\n",
    "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    cited_urls = set(generated[\"parsed\"].cited_urls)\n",
    "    # Save the retrieved information to a the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    formatted_message = AIMessage(name=name, content=generated[\"parsed\"].as_str)\n",
    "    return {\"messages\": [formatted_message], \"references\": cited_references}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Large language models with million-plus token context windows, such as Gemini 1.5, have generated discussions in the AI community regarding their impact on the Retrieval-Augmented Generation (RAG) framework. These models are believed to potentially have a negative effect on RAG^(1). The RAG framework typically involves components like Milvus as the vector database, LangChain as the orchestrator, and large language models like GTE-Large for text generation^(2). While large context windows are desirable in language models, the high fine-tuning costs, scarcity of long texts, and challenges like catastrophic values introduced by new token positions limit the current extended context windows to around 128k tokens^(3). Recent advancements, like LongRoPE, have extended the context window of pre-trained large language models significantly to 2048k tokens^(3). The integration of retrieval mechanisms with long context language models, such as GPT-3.5-Turbo-16k and Llama2-7B-chat-4k, has been explored to evaluate the impact of retrieval on model performance in the RAG framework^(4). Challenges in integrating large language models into RAG include inaccuracies, cost-efficiency, and the need for optimization through vector databases^(5). RAG combines large language models with retrieval modules to ground text generation in knowledge, addressing challenges like inaccuracies and cost-efficiency^(6). The RAG framework involves indexing external data sources, converting them into vector embeddings, and retrieval processes to enhance the capabilities of large language models like GPT-3 or BERT^(7). While RAG offers flexibility and scalability in integrating with existing AI systems, challenges persist in ensuring the accuracy of external data sources^(8). Large language models like GPT-4 have demonstrated impressive text generation abilities, but challenges remain in retaining factual knowledge^(9).\\n\\nCitations:\\n\\n[1]: https://medium.com/enterprise-rag/why-gemini-1-5-and-other-large-context-models-are-bullish-for-rag-ce3218930bb4\\n[2]: https://zilliz.com/blog/building-rag-without-openai-mixtral-milvus-octoai\\n[3]: https://arxiv.org/abs/2402.13753\\n[4]: https://blog.llamaindex.ai/nvidia-research-rag-with-long-context-llms-7d94d40090c4\\n[5]: https://www.infoworld.com/article/3712227/what-is-rag-more-accurate-and-reliable-llms.html\\n[6]: https://medium.com/@juanc.olamendy/rag-best-practices-enhancing-large-language-models-with-retrieval-augmented-generation-6961c8b834ff\\n[7]: https://www.deepset.ai/blog/generative-llm-evaluation-rag\\n[8]: https://ai88.substack.com/p/rag-vs-context-window-in-gpt4-accuracy-cost\\n[9]: https://wetheitguys.medium.com/understanding-retrieval-augmented-generation-rag-with-large-language-models-llms-b77c76b9f9d8\\n[10]: https://medium.com/@bijit211987/optimizing-rag-for-llms-apps-53f6056d8118'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_answer = await gen_answer(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "example_answer[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 5\n",
    "\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"Subject Matter Expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "    if num_responses >= max_num_turns:\n",
    "        return END\n",
    "    last_question = messages[-2]\n",
    "    if last_question.content.endswith(\"Thank you so much for your help!\"):\n",
    "        return END\n",
    "    return \"ask_question\"\n",
    "\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "\n",
    "builder.add_node(\"ask_question\", generate_question)\n",
    "builder.add_node(\"answer_question\", gen_answer)\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "builder.set_entry_point(\"ask_question\")\n",
    "interview_graph = builder.compile().with_config(run_name=\"Conduct Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_question\n",
      "--  [AIMessage(content=\"Yes, that's correct. I am focusing on analyzing the impact of million-plus token context window language models on the RAG (Retrieval-Augmented Generation) framework. This involves looking at how these large language models affect the efficiency, effectiveness, and potential chal\n",
      "answer_question\n",
      "--  [AIMessage(content='The impact of million-plus token context window language models on the RAG (Retrieval-Augmented Generation) framework has been a topic of discussion in the AI community. For example, the introduction of Gemini 1.5, with a 1 million token context window, has raised concerns about \n",
      "ask_question\n",
      "--  [AIMessage(content='Thank you for providing such detailed information and relevant citations. Could you elaborate on the specific challenges researchers have encountered when integrating million-plus token context window language models into the RAG framework, and how they have attempted to address \n",
      "answer_question\n",
      "--  [AIMessage(content=\"Integrating million-plus token context window language models into the RAG (Retrieval-Augmented Generation) framework poses challenges such as high fine-tuning costs, scarcity of long texts, and the introduction of catastrophic values by new token positions, limiting the extended\n",
      "ask_question\n",
      "--  [AIMessage(content='Thank you for sharing this insightful information about the challenges and advancements in integrating million-plus token context window language models into the RAG framework. Can you provide more details on how RAG leverages external knowledge sources through retrieval to enhan\n",
      "answer_question\n",
      "--  [AIMessage(content='Retrieval-Augmented Generation (RAG) is a technique that enhances the accuracy and reliability of generative AI models, such as Large Language Models (LLMs), by incorporating facts fetched from external sources. RAG leverages external knowledge sources through retrieval to addres\n",
      "ask_question\n",
      "--  [AIMessage(content='Thank you for providing a concise summary of how Retrieval-Augmented Generation (RAG) leverages external knowledge sources through retrieval to enhance the accuracy and credibility of Large Language Models (LLMs). This information will be valuable for my research and editing of t\n",
      "answer_question\n",
      "--  [AIMessage(content='RAG leverages external knowledge sources through retrieval to address challenges like hallucination, outdated knowledge, and non-transparent reasoning processes. By integrating information from external databases, RAG improves the credibility and accuracy of LLMs, making them mor\n",
      "__end__\n",
      "--  [AIMessage(content='So you said you were writing an article on Impact of million-plus token context window language models on RAG?', name='Subject Matter Expert'), AIMessage(content=\"Yes, that's correct. I am focusing on analyzing the impact of million-plus token context window language models on th\n"
     ]
    }
   ],
   "source": [
    "final_step = None\n",
    "\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=f\"So you said you were writing an article on {example_topic}?\",\n",
    "            name=\"Subject Matter Expert\",\n",
    "        )\n",
    "    ],\n",
    "}\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name][\"messages\"])[:300])\n",
    "    if END in step:\n",
    "        final_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = next(iter(final_step.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine Outline\n",
    "\n",
    "Now that we have all this cool stuff, let's distill it into a refined outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a Wikipedia writer. You have gathered information from experts and search engines. Now, you are refining the outline of the Wikipedia page.\\\n",
    "You need to make sure that the outline is comprehensive and specific.\\\n",
    "Topic you are writing about: {topic}\\\n",
    "Old outline: {old_outline}\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Refine the outline based on your conversations with subject-matter experts:\\n\\nConversations:\\n\\n{conversations}\\n\\nWrite the refined Wikipedia outline:\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Using turbo preview since the context can get quite long\n",
    "refine_outline_chain = refine_outline_prompt | long_context_llm.with_structured_output(\n",
    "    Outline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_outline = refine_outline_chain.invoke(\n",
    "    {\n",
    "        \"topic\": example_topic,\n",
    "        \"old_outline\": initial_outline.as_str,\n",
    "        \"conversations\": \"\\n\\n\".join(\n",
    "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of Million-Plus Token Context Window Language Models on RAG\n",
      "\n",
      "## Introduction\n",
      "\n",
      "An overview of the development and significance of million-plus token context window language models and the concept of Retrieval-Augmented Generation (RAG).\n",
      "\n",
      "## The Evolution of Large Context Windows in Language Models\n",
      "\n",
      "A historical perspective on the growth of context window sizes in language models, including key milestones such as Gemini 1.5, Mixtral, GPT-3.5-Turbo-16k, Llama2-7B-chat-4k, and LongRoPE.\n",
      "\n",
      "### Key Milestones\n",
      "\n",
      "Discussion of significant advancements and models that have shaped the current landscape of large context window language models.\n",
      "\n",
      "### Challenges Overcome\n",
      "\n",
      "Examination of the technical and theoretical hurdles encountered in expanding the context window sizes of language models.\n",
      "\n",
      "## Integration Challenges with RAG\n",
      "\n",
      "Detailed analysis of the specific challenges faced when integrating million-plus token context window language models into the RAG framework, such as high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions.\n",
      "\n",
      "### Addressing High Fine-Tuning Costs\n",
      "\n",
      "Exploration of strategies and technological advancements aimed at reducing the high fine-tuning costs associated with large context windows.\n",
      "\n",
      "### Overcoming Scarcity of Long Texts\n",
      "\n",
      "Discussion on methods to mitigate the scarcity of long texts suitable for training and leveraging million-plus token models.\n",
      "\n",
      "### Mitigating Catastrophic Values\n",
      "\n",
      "Analysis of approaches to limit the impact of catastrophic values introduced by new token positions in extended context windows.\n",
      "\n",
      "## Enhancing RAG with External Knowledge Sources\n",
      "\n",
      "Insight into how RAG leverages external knowledge sources through retrieval to address challenges like hallucination, outdated knowledge, and non-transparent reasoning processes, thereby enhancing the accuracy and credibility of LLMs.\n",
      "\n",
      "### Improving Credibility and Accuracy\n",
      "\n",
      "Discussion on the importance of integrating external data sources to enhance the credibility and accuracy of responses generated by LLMs.\n",
      "\n",
      "### Addressing Hallucination and Outdated Knowledge\n",
      "\n",
      "Analysis of how RAG's retrieval mechanism helps in minimizing issues of hallucination and outdated knowledge by providing up-to-date and relevant information.\n",
      "\n",
      "## Applications and Future Directions\n",
      "\n",
      "Exploration of potential applications for RAG integrated with million-plus token context window language models and speculation on future developments in this area.\n"
     ]
    }
   ],
   "source": [
    "print(refined_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Article\n",
    "\n",
    "Now it's time to generate the full article. We will divide-and-conquer, so that each section can be tackled by an individual llm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubSection(BaseModel):\n",
    "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
    "    content: str = Field(\n",
    "        ...,\n",
    "        title=\"Full content of the subsection. Include [#] citations to the cited sources where relevant.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"### {self.subsection_title}\\n\\n{self.content}\".strip()\n",
    "\n",
    "\n",
    "class WikiSection(BaseModel):\n",
    "    section_title: str = Field(..., title=\"Title of the section\")\n",
    "    content: str = Field(..., title=\"Full content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
    "    )\n",
    "    citations: List[str] = Field(default_factory=list)\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            subsection.as_str for subsection in self.subsections or []\n",
    "        )\n",
    "        citations = \"\\n\".join([f\" [{i}] {cit}\" for i, cit in enumerate(self.citations)])\n",
    "        return (\n",
    "            f\"## {self.section_title}\\n\\n{self.content}\\n\\n{subsections}\".strip()\n",
    "            + f\"\\n\\n{citations}\".strip()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "reference_docs = [\n",
    "    Document(page_content=v, metadata={\"source\": k})\n",
    "    for k, v in final_state[\"references\"].items()\n",
    "]\n",
    "# This really doesn't need to be a vectorstore.\n",
    "# could just be a numpy matrix\n",
    "vectorstore = SKLearnVectorStore.from_documents(\n",
    "    reference_docs,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Large Language Models (LLMs) have achieved remarkable success across various tasks. However, they often grapple with a limited context window size due to the high costs of fine-tuning, scarcity of lengthy texts, and the introduction of catastrophic values by new token positions. To address this issue, in a new paper LongRoPE: Extending LLM Context Window', metadata={'id': '20dbbce3-ae12-4a05-94e9-df4a97676098', 'source': 'https://syncedreview.com/2024/02/25/microsofts-longrope-breaks-the-limit-of-context-window-of-llms-extents-it-to-2-million-tokens/'}),\n",
       " Document(page_content='Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k ...', metadata={'id': 'f7881b24-697b-46fa-a31c-8e476ae40f3f', 'source': 'https://arxiv.org/abs/2402.13753'}),\n",
       " Document(page_content='Large language models (LLMs) have witnessed significant advancements, aiming to enhance their capabilities for interpreting and processing extensive textual data. LLMs like GPT-3 have revolutionized our interactions with AI, offering insights and analyses across various domains, from writing assistance to complex data interpretation. However, a key limitation has been their context window size ...', metadata={'id': '354ceab3-03ae-4141-94a1-4b0109182aab', 'source': 'https://www.marktechpost.com/2024/02/23/breaking-barriers-in-language-understanding-how-microsoft-ais-longrope-extends-large-language-models-to-a-2048k-token-context-window/'}),\n",
       " Document(page_content='Large Language Models (LLMs) demonstrate significant capabilities but face challenges such as hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the models, particularly for knowledge ...', metadata={'id': '85f7d7a1-2820-466c-ac80-975f4e2d65af', 'source': 'https://arxiv.org/abs/2312.10997'})]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What's a long context LLM anyway?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "section_writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia writer. Complete your assigned WikiSection from the following outline:\\n\\n\"\n",
    "            \"{outline}\\n\\nCite your sources, using the following references:\\n\\n<Documents>\\n{docs}\\n<Documents>\",\n",
    "        ),\n",
    "        (\"user\", \"Write the full WikiSection for the {section} section.\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "async def retrieve(inputs: dict):\n",
    "    docs = await retriever.ainvoke(inputs[\"topic\"] + \": \" + inputs[\"section\"])\n",
    "    formatted = \"\\n\".join(\n",
    "        [\n",
    "            f'<Document href=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )\n",
    "    return {\"docs\": formatted, **inputs}\n",
    "\n",
    "\n",
    "section_writer = (\n",
    "    retrieve\n",
    "    | section_writer_prompt\n",
    "    | long_context_llm.with_structured_output(WikiSection)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impact of million-plus token context window language models on RAG The Evolution of Large Context Windows in Language Models\n",
      "## The Evolution of Large Context Windows in Language Models\n",
      "\n",
      "The evolution of large context windows in language models (LLMs) has been a critical factor in the advancement of natural language processing (NLP) technologies. Initially, LLMs were constrained by smaller context windows, limiting their understanding and generation capabilities. However, the demand for models capable of processing and integrating more extensive sequences of text has led to significant research and development efforts aimed at expanding these context windows.\n",
      "\n",
      "Over time, this push for larger context windows has seen the emergence of several key milestones that have progressively increased the amount of text LLMs can consider when generating responses or analyses. These milestones include models like Gemini 1.5, Mixtral, GPT-3.5-Turbo-16k, Llama2-7B-chat-4k, and LongRoPE, each contributing to the landscape of large context window LLMs in unique ways.\n",
      "\n",
      "Despite the benefits, expanding the context window size brings several challenges, including the high fine-tuning costs associated with processing longer sequences of text, the scarcity of long texts suitable for training these models, and the potential for catastrophic values introduced by new token positions in expanded contexts. Addressing these challenges has been central to the continued development and effectiveness of LLMs with large context windows.\n",
      "\n",
      "### Key Milestones\n",
      "\n",
      "The journey towards expanding the context window sizes of language models has been marked by several significant milestones. Gemini 1.5, Mixtral, GPT-3.5-Turbo-16k, Llama2-7B-chat-4k, and LongRoPE represent some of the most notable advancements in this area. Each of these models has pushed the boundaries of what was previously possible, setting new standards for the amount of text that can be processed and understood by LLMs.\n",
      "\n",
      "For instance, LongRoPE has made a groundbreaking contribution by extending the context window of pre-trained LLMs to an impressive 2048k tokens, far surpassing previous limits and opening up new possibilities for NLP applications.\n",
      "\n",
      "### Challenges Overcome\n",
      "\n",
      "Expanding the context window sizes of language models has not been without its challenges. High fine-tuning costs, the scarcity of suitable long texts for training, and the introduction of catastrophic values by new token positions have all posed significant hurdles.\n",
      "\n",
      "Technological and methodological advancements have been crucial in overcoming these challenges, allowing for the successful expansion of context windows beyond previous limitations. Innovations in model architecture, training methodologies, and data processing techniques have all played a role in addressing these issues and enabling the development of more capable and efficient LLMs.[0] https://arxiv.org/abs/2402.13753\n",
      " [1] https://syncedreview.com/2024/02/25/microsofts-longrope-breaks-the-limit-of-context-window-of-llms-extents-it-to-2-million-tokens/\n",
      " [2] https://www.marktechpost.com/2024/02/23/breaking-barriers-in-language-understanding-how-microsoft-ais-longrope-extends-large-language-models-to-a-2048k-token-context-window/\n"
     ]
    }
   ],
   "source": [
    "section = await section_writer.ainvoke(\n",
    "    {\n",
    "        \"outline\": refined_outline.as_str,\n",
    "        \"section\": refined_outline.sections[1].section_title,\n",
    "        \"topic\": example_topic,\n",
    "    }\n",
    ")\n",
    "print(section.as_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Evolution of Large Context Windows in Language Models\n",
      "\n",
      "The evolution of large context windows in language models (LLMs) has been a critical factor in the advancement of natural language processing (NLP) technologies. Initially, LLMs were constrained by smaller context windows, limiting their understanding and generation capabilities. However, the demand for models capable of processing and integrating more extensive sequences of text has led to significant research and development efforts aimed at expanding these context windows.\n",
      "\n",
      "Over time, this push for larger context windows has seen the emergence of several key milestones that have progressively increased the amount of text LLMs can consider when generating responses or analyses. These milestones include models like Gemini 1.5, Mixtral, GPT-3.5-Turbo-16k, Llama2-7B-chat-4k, and LongRoPE, each contributing to the landscape of large context window LLMs in unique ways.\n",
      "\n",
      "Despite the benefits, expanding the context window size brings several challenges, including the high fine-tuning costs associated with processing longer sequences of text, the scarcity of long texts suitable for training these models, and the potential for catastrophic values introduced by new token positions in expanded contexts. Addressing these challenges has been central to the continued development and effectiveness of LLMs with large context windows.\n",
      "\n",
      "### Key Milestones\n",
      "\n",
      "The journey towards expanding the context window sizes of language models has been marked by several significant milestones. Gemini 1.5, Mixtral, GPT-3.5-Turbo-16k, Llama2-7B-chat-4k, and LongRoPE represent some of the most notable advancements in this area. Each of these models has pushed the boundaries of what was previously possible, setting new standards for the amount of text that can be processed and understood by LLMs.\n",
      "\n",
      "For instance, LongRoPE has made a groundbreaking contribution by extending the context window of pre-trained LLMs to an impressive 2048k tokens, far surpassing previous limits and opening up new possibilities for NLP applications.\n",
      "\n",
      "### Challenges Overcome\n",
      "\n",
      "Expanding the context window sizes of language models has not been without its challenges. High fine-tuning costs, the scarcity of suitable long texts for training, and the introduction of catastrophic values by new token positions have all posed significant hurdles.\n",
      "\n",
      "Technological and methodological advancements have been crucial in overcoming these challenges, allowing for the successful expansion of context windows beyond previous limitations. Innovations in model architecture, training methodologies, and data processing techniques have all played a role in addressing these issues and enabling the development of more capable and efficient LLMs.[0] https://arxiv.org/abs/2402.13753\n",
      " [1] https://syncedreview.com/2024/02/25/microsofts-longrope-breaks-the-limit-of-context-window-of-llms-extents-it-to-2-million-tokens/\n",
      " [2] https://www.marktechpost.com/2024/02/23/breaking-barriers-in-language-understanding-how-microsoft-ais-longrope-extends-large-language-models-to-a-2048k-token-context-window/\n"
     ]
    }
   ],
   "source": [
    "print(section.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate final article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "writer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert Wikipedia author. Write the complete wiki article on {topic} using the following section drafts:\\n\\n\"\n",
    "            \"{draft}\\n\\nStrictly follow Wikipedia format guidelines.\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            'Write the complete Wiki article using markdown format. Organize citations using footnotes like \"[1]\", avoiding duplicates in the footer.',\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "writer = writer_prompt | long_context_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of Million-Plus Token Context Window Language Models on RAG\n",
      "\n",
      "The development and implementation of million-plus token context window language models (LLMs) represent a significant milestone in the field of natural language processing (NLP). These models have dramatically enhanced the capabilities of retrieval-augmented generation (RAG) systems, enabling them to generate more accurate, contextually relevant, and nuanced text outputs. This article delves into the evolution of large context windows in LLMs, their impact on RAG, and the challenges faced along the way.\n",
      "\n",
      "## Contents\n",
      "\n",
      "1. [The Evolution of Large Context Windows in Language Models](#The-Evolution-of-Large-Context-Windows-in-Language-Models)\n",
      "    1. [Key Milestones](#Key-Milestones)\n",
      "    2. [Challenges Overcome](#Challenges-Overcome)\n",
      "2. [Impact on Retrieval-Augmented Generation](#Impact-on-Retrieval-Augmented-Generation)\n",
      "    1. [Enhanced Contextual Understanding](#Enhanced-Contextual-Understanding)\n",
      "    2. [Improved Accuracy and Relevance](#Improved-Accuracy-and-Relevance)\n",
      "    3. [Challenges and Solutions](#Challenges-and-Solutions)\n",
      "3. [Conclusion](#Conclusion)\n",
      "4. [References](#References)\n",
      "\n",
      "## The Evolution of Large Context Windows in Language Models\n",
      "\n",
      "The evolution of large context windows in language models (LLMs) has been instrumental in advancing natural language processing (NLP) technologies. Initially, these models were restricted by smaller context windows, limiting their comprehension and generation abilities. The push for models that could process and integrate longer text sequences led to significant research and development efforts aimed at expanding these context windows.\n",
      "\n",
      "### Key Milestones\n",
      "\n",
      "Several key milestones have marked the journey towards enlarging the context window sizes of language models, including Gemini 1.5, Mixtral, GPT-3.5-Turbo-16k, Llama2-7B-chat-4k, and LongRoPE. Each model has contributed uniquely to the landscape of large context window LLMs, progressively increasing the amount of text that LLMs can consider when generating responses or analyses. Notably, LongRoPE has been a groundbreaking advancement, extending the context window to an impressive 2048k tokens[1][2].\n",
      "\n",
      "### Challenges Overcome\n",
      "\n",
      "The expansion of context window sizes has encountered several challenges, such as high fine-tuning costs, the scarcity of suitable long texts for training, and the introduction of catastrophic values by new token positions. Technological and methodological advancements have been pivotal in overcoming these challenges, enabling the successful expansion of context windows[0].\n",
      "\n",
      "## Impact on Retrieval-Augmented Generation\n",
      "\n",
      "The implementation of million-plus token context window LLMs has had a profound impact on RAG systems, enhancing their performance in various ways.\n",
      "\n",
      "### Enhanced Contextual Understanding\n",
      "\n",
      "With the ability to process and integrate larger sequences of text, RAG systems can now generate responses that are more contextually relevant and nuanced. This has significantly improved the quality of text generation across diverse NLP applications.\n",
      "\n",
      "### Improved Accuracy and Relevance\n",
      "\n",
      "The expanded context windows allow RAG systems to retrieve and leverage more relevant information, leading to improvements in the accuracy and relevance of generated text. This capability is particularly beneficial in tasks that require a deep understanding of large text corpora, such as summarization and question-answering.\n",
      "\n",
      "### Challenges and Solutions\n",
      "\n",
      "Despite the benefits, integrating million-plus token context window LLMs into RAG systems presents challenges, including increased computational requirements and the need for more sophisticated retrieval mechanisms. Ongoing research and development are focused on addressing these issues, ensuring the continued advancement of RAG technologies.\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "The advent of million-plus token context window LLMs has been a game-changer for RAG systems, significantly enhancing their text generation capabilities. Despite the challenges, the benefits of expanded context windows are undeniable, paving the way for more sophisticated and capable NLP applications.\n",
      "\n",
      "## References\n",
      "\n",
      "[0] \"Techniques for Expanding Context Window Sizes in Language Models,\" arXiv, 2024. https://arxiv.org/abs/2402.13753\n",
      "\n",
      "[1] \"Microsoft's LongRoPE Breaks the Limit of Context Window of LLMs, Extends it to 2 Million Tokens,\" SyncedReview, 2024. https://syncedreview.com/2024/02/25/microsofts-longrope-breaks-the-limit-of-context-window-of-llms-extents-it-to-2-million-tokens/\n",
      "\n",
      "[2] \"Breaking Barriers in Language Understanding: How Microsoft AI's LongRoPE Extends Large Language Models to a 2048k Token Context Window,\" MarkTechPost, 2024. https://www.marktechpost.com/2024/02/23/breaking-barriers-in-language-understanding-how-microsoft-ais-longrope-extends-large-language-models-to-a-2048k-token-context-window/"
     ]
    }
   ],
   "source": [
    "for tok in writer.stream({\"topic\": example_topic, \"draft\": section.as_str}):\n",
    "    print(tok, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Flow\n",
    "\n",
    "Now it's time to string everything together. We will have 3 main stages in sequence:\n",
    ".\n",
    "1. Generate the initial outline + perspectives\n",
    "2. Batch converse with each perspective to expand the content for the article.\n",
    "3. Refine the outline based on the conversations\n",
    "4. Write the final wiki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    outline: Outline\n",
    "    editors: List[Editor]\n",
    "    interview_results: List[InterviewState]\n",
    "    # The final sections output\n",
    "    sections: List[WikiSection]\n",
    "    article: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "\n",
    "async def initialize_research(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    coros = (\n",
    "        generate_outline_direct.ainvoke({\"topic\": topic}),\n",
    "        survey_subjects.ainvoke(topic),\n",
    "    )\n",
    "    results = await asyncio.gather(*coros)\n",
    "    return {\n",
    "        **state,\n",
    "        \"outline\": results[0],\n",
    "        \"editors\": results[1].editors,\n",
    "    }\n",
    "\n",
    "\n",
    "async def conduct_interviews(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    initial_states = [\n",
    "        {\n",
    "            \"editor\": editor,\n",
    "            \"messages\": [\n",
    "                AIMessage(\n",
    "                    content=f\"So you said you were writing an article on {topic}?\",\n",
    "                    name=\"Subject Matter Expert\",\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        for editor in state[\"editors\"]\n",
    "    ]\n",
    "    # We call in to the sub-graph here\n",
    "    interview_results = await interview_graph.abatch(initial_states)\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"interview_results\": interview_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_conversation(interview_state):\n",
    "    messages = interview_state[\"messages\"]\n",
    "    convo = \"\\n\".join(f\"{m.name}: {m.content}\" for m in final_state[\"messages\"])\n",
    "    return f'Conversation with {interview_state[\"editor\"].name}\\n\\n' + convo\n",
    "\n",
    "\n",
    "async def refine_outline(state: ResearchState):\n",
    "    convos = \"\\n\\n\".join(\n",
    "        [\n",
    "            format_conversation(interview_state)\n",
    "            for interview_state in state[\"interview_results\"]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    updated_outline = await refine_outline_chain.ainvoke(\n",
    "        {\n",
    "            \"topic\": state[\"topic\"],\n",
    "            \"old_outline\": state[\"outline\"].as_str,\n",
    "            \"conversations\": convos,\n",
    "        }\n",
    "    )\n",
    "    return {**state, \"outline\": updated_outline}\n",
    "\n",
    "\n",
    "async def index_references(state: ResearchState):\n",
    "    all_docs = []\n",
    "    for interview_state in state[\"interview_results\"]:\n",
    "        reference_docs = [\n",
    "            Document(page_content=v, metadata={\"source\": k})\n",
    "            for k, v in interview_state[\"references\"].items()\n",
    "        ]\n",
    "        all_docs.extend(reference_docs)\n",
    "    await vectorstore.aadd_documents(all_docs)\n",
    "    return state\n",
    "\n",
    "\n",
    "async def write_sections(state: ResearchState):\n",
    "    outline = state[\"outline\"]\n",
    "    sections = await section_writer.abatch(\n",
    "        [\n",
    "            {\n",
    "                \"outline\": refined_outline.as_str,\n",
    "                \"section\": section.section_title,\n",
    "                \"topic\": state[\"topic\"],\n",
    "            }\n",
    "            for section in outline.sections\n",
    "        ]\n",
    "    )\n",
    "    return {\n",
    "        **state,\n",
    "        \"sections\": sections,\n",
    "    }\n",
    "\n",
    "\n",
    "async def write_article(state: ResearchState):\n",
    "    topic = state[\"topic\"]\n",
    "    sections = state[\"sections\"]\n",
    "    draft = \"\\n\\n\".join([section.as_str for section in sections])\n",
    "    article = await writer.ainvoke({\"topic\": example_topic, \"draft\": draft})\n",
    "    return {\n",
    "        **state,\n",
    "        \"article\": article,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_of_storm = StateGraph(ResearchState)\n",
    "\n",
    "\n",
    "nodes = [\n",
    "    (\"init_research\", initialize_research),\n",
    "    (\"conduct_interviews\", conduct_interviews),\n",
    "    (\"refine_outline\", refine_outline),\n",
    "    (\"index_references\", index_references),\n",
    "    (\"write_sections\", write_sections),\n",
    "    (\"write_article\", write_article),\n",
    "]\n",
    "for i in range(len(nodes)):\n",
    "    name, node = nodes[i]\n",
    "    builder_of_storm.add_node(name, node)\n",
    "    if i > 0:\n",
    "        builder_of_storm.add_edge(nodes[i - 1][0], name)\n",
    "\n",
    "builder_of_storm.set_entry_point(nodes[0][0])\n",
    "builder_of_storm.set_finish_point(nodes[-1][0])\n",
    "storm = builder_of_storm.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_research\n",
      "--  {'topic': 'NVIDIA 2024 Q1 earnings report', 'outline': Outline(page_title='NVIDIA 2024 Q1 Earnings Report', sections=[Section(section_title='Overview', description='Brief introduction to NVIDIA and an overview of the 2024 Q1 earnings report', subsections=None), Section(section_title='Financial Perfo\n",
      "conduct_interviews\n",
      "--  {'topic': 'NVIDIA 2024 Q1 earnings report', 'outline': Outline(page_title='NVIDIA 2024 Q1 Earnings Report', sections=[Section(section_title='Overview', description='Brief introduction to NVIDIA and an overview of the 2024 Q1 earnings report', subsections=None), Section(section_title='Financial Perfo\n",
      "refine_outline\n",
      "--  {'topic': 'NVIDIA 2024 Q1 earnings report', 'outline': Outline(page_title='Impact of Million-Plus Token Context Window Language Models on RAG', sections=[Section(section_title='Introduction', description='An overview of the article, including the significance of million-plus token context window lan\n",
      "index_references\n",
      "--  {'topic': 'NVIDIA 2024 Q1 earnings report', 'outline': Outline(page_title='Impact of Million-Plus Token Context Window Language Models on RAG', sections=[Section(section_title='Introduction', description='An overview of the article, including the significance of million-plus token context window lan\n",
      "NVIDIA 2024 Q1 earnings report Introduction\n",
      "NVIDIA 2024 Q1 earnings report Background\n",
      "NVIDIA 2024 Q1 earnings report Impact of Large Context Windows on RAG\n",
      "NVIDIA 2024 Q1 earnings report Case Studies\n",
      "NVIDIA 2024 Q1 earnings report RAG's Use of External Knowledge Sources\n",
      "NVIDIA 2024 Q1 earnings report Future Outlook\n",
      "NVIDIA 2024 Q1 earnings report Conclusions\n",
      "NVIDIA 2024 Q1 earnings report References\n",
      "write_sections\n",
      "--  {'topic': 'NVIDIA 2024 Q1 earnings report', 'outline': Outline(page_title='Impact of Million-Plus Token Context Window Language Models on RAG', sections=[Section(section_title='Introduction', description='An overview of the article, including the significance of million-plus token context window lan\n",
      "write_article\n",
      "--  {'topic': 'NVIDIA 2024 Q1 earnings report', 'outline': Outline(page_title='Impact of Million-Plus Token Context Window Language Models on RAG', sections=[Section(section_title='Introduction', description='An overview of the article, including the significance of million-plus token context window lan\n",
      "__end__\n",
      "--  {'topic': 'NVIDIA 2024 Q1 earnings report', 'outline': Outline(page_title='Impact of Million-Plus Token Context Window Language Models on RAG', sections=[Section(section_title='Introduction', description='An overview of the article, including the significance of million-plus token context window lan\n"
     ]
    }
   ],
   "source": [
    "async for step in storm.astream(\n",
    "    {\n",
    "        \"topic\": \"NVIDIA 2024 Q1 earnings report\",\n",
    "    }\n",
    "):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"-- \", str(step[name])[:300])\n",
    "    if END in step:\n",
    "        results = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = results[END][\"article\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Impact of Million-Plus Token Context Window Language Models on RAG\n",
      "\n",
      "The **Impact of Million-Plus Token Context Window Language Models on Retrieval-Augmented Generation (RAG)** reflects a significant advancement in the field of natural language processing (NLP) and artificial intelligence (AI). This development has broadened the capabilities of AI systems in understanding and generating human-like text by integrating large-scale language models with external knowledge sources to produce contextually rich responses.\n",
      "\n",
      "## Contents\n",
      "\n",
      "- [Introduction](#Introduction)\n",
      "- [Background](#Background)\n",
      "- [Impact of Million-Plus Token Context Window Language Models on RAG](#Impact-of-Million-Plus-Token-Context-Window-Language-Models-on-RAG)\n",
      "  - [The Evolution of Large Context Windows in Language Models](#The-Evolution-of-Large-Context-Windows-in-Language-Models)\n",
      "  - [Integration Challenges with RAG](#Integration-Challenges-with-RAG)\n",
      "  - [Enhancing RAG with External Knowledge Sources](#Enhancing-RAG-with-External-Knowledge-Sources)\n",
      "  - [Applications and Future Directions](#Applications-and-Future-Directions)\n",
      "- [Case Studies](#Case-Studies)\n",
      "- [Conclusions](#Conclusions)\n",
      "- [References](#References)\n",
      "\n",
      "## Introduction\n",
      "\n",
      "The advent of million-plus token context window language models marks a significant milestone in NLP and AI. These models process and understand vast amounts of text, enhancing machine learning applications, particularly in text generation and comprehension. The integration within the RAG framework improves accuracy, relevancy, and contextual richness of responses, addressing limitations of traditional language models.\n",
      "\n",
      "## Background\n",
      "\n",
      "The development of large language models (LLMs) with increasing parameter sizes and context windows has enhanced their capability to generate coherent and contextually relevant text outputs. The expansion of the context window has been crucial for understanding and generating complex texts. RAG, by integrating external knowledge sources, allows LLMs to provide more accurate and updated responses, opening new possibilities in NLP.\n",
      "\n",
      "## Impact of Million-Plus Token Context Window Language Models on RAG\n",
      "\n",
      "### The Evolution of Large Context Windows in Language Models\n",
      "Models like Gemini 1.5, Mixtral, and GPT-3.5-Turbo-16k have pushed technical boundaries, addressing challenges in increasing context window size, enhancing understanding and text generation capabilities.\n",
      "\n",
      "### Integration Challenges with RAG\n",
      "Integrating these models into RAG frameworks introduces challenges such as high fine-tuning costs and managing catastrophic values. Technological innovations and approaches to mitigate these effects are crucial for successful integration.\n",
      "\n",
      "### Enhancing RAG with External Knowledge Sources\n",
      "The integration benefits RAG by addressing challenges like hallucination and outdated knowledge, leveraging up-to-date information to enhance response accuracy and credibility.\n",
      "\n",
      "### Applications and Future Directions\n",
      "This integration opens up applications in chatbots, content creation, and information retrieval. Future developments aim to improve these models' efficiency, accuracy, and application breadth.\n",
      "\n",
      "## Case Studies\n",
      "\n",
      "Specific case studies demonstrate practical applications and challenges in integrating advanced language models with RAG, highlighting improvements in retrieval capabilities, data efficiency, and addressing information overload.\n",
      "\n",
      "## Conclusions\n",
      "\n",
      "The integration of million-plus token context window language models with RAG has significantly advanced NLP capabilities, enhancing machine understanding and generation of human language. Despite challenges, technological and strategic advancements continue to push the boundaries of what is possible in NLP, promising more sophisticated and nuanced AI-driven technologies.\n",
      "\n",
      "## References\n",
      "\n",
      "1. NVIDIA Announces Financial Results for First Quarter Fiscal 2024. [NVIDIA News](https://nvidianews.nvidia.com/news/nvidia-announces-financial-results-for-first-quarter-fiscal-2024)\n",
      "\n",
      "2. Nvidia Q1 Earnings Report. [InvestorPlace](https://investorplace.com/market360/2023/05/nvidia-q1-earnings-report/)\n"
     ]
    }
   ],
   "source": [
    "print(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
