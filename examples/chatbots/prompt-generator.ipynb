{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0aa20b8-fe59-4c91-a06f-32002f142a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "template = \"\"\"Your job is to get information from a user about what type of prompt template they want to create.\n",
    "\n",
    "You should get the following information from them:\n",
    "\n",
    "- What the objective of the prompt is\n",
    "- What variables will be passed into the prompt template\n",
    "- Any constraints for what the output should NOT do\n",
    "- Any requirements that the output MUST adhere to\n",
    "\n",
    "If you are not able to discerne this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "\n",
    "After you are able to discerne all the information, call the relevant tool\"\"\"\n",
    "def get_messages_info(messages):\n",
    "    return [SystemMessage(content=template)] + messages\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class PromptInstructions(BaseModel):\n",
    "    \"\"\"Instructions on how to prompt the LLM.\"\"\"\n",
    "    objective: str\n",
    "    variables: List[str]\n",
    "    constraints: List[str]\n",
    "    requirements: List[str]\n",
    "\n",
    "llm_with_tool = llm.bind_tools([PromptInstructions])\n",
    "\n",
    "chain = get_messages_info | llm_with_tool\n",
    "\n",
    "def _is_tool_call(msg):\n",
    "    return hasattr(msg, \"additional_kwargs\") and 'tool_calls' in msg.additional_kwargs\n",
    "\n",
    "prompt_system = \"\"\"Based on the following requirements, write a good prompt template:\n",
    "\n",
    "{reqs}\"\"\"\n",
    "def get_prompt_messages(messages):\n",
    "    tool_call = None\n",
    "    other_msgs = []\n",
    "    for m in messages:\n",
    "        if _is_tool_call(m):\n",
    "            tool_call = m.additional_kwargs['tool_calls'][0]['function']['arguments']\n",
    "        elif tool_call is not None:\n",
    "            other_msgs.append(m)\n",
    "    return [SystemMessage(content=prompt_system.format(reqs=tool_call))] + other_msgs\n",
    "    \n",
    "\n",
    "prompt_gen_chain = get_prompt_messages | llm\n",
    "\n",
    "def get_state(messages):\n",
    "    if _is_tool_call(messages[-1]):\n",
    "        return \"prompt\"\n",
    "    elif not isinstance(messages[-1], HumanMessage):\n",
    "        return END\n",
    "    for m in messages:\n",
    "        if _is_tool_call(m):\n",
    "            return \"prompt\"\n",
    "    return \"info\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d9d6b4-dce4-43cc-9a1a-61a7912ed5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph, END\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "nodes = {k:k for k in ['info', 'prompt', END]}\n",
    "workflow = MessageGraph()\n",
    "workflow.add_node(\"info\", chain)\n",
    "workflow.add_node(\"prompt\", prompt_gen_chain)\n",
    "workflow.add_conditional_edges(\"info\", get_state, nodes)\n",
    "workflow.add_conditional_edges(\"prompt\", get_state, nodes)\n",
    "workflow.set_entry_point(\"info\")\n",
    "graph = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25793988-45a2-4e65-b33c-64e72aadb10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  hi!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='Hello! How can I assist you today?'\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  build me a prompt for extraction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='Sure! I can help you with that. Could you please provide me with more details about the prompt you want to create? Specifically, I need to know the objective of the prompt, the variables that will be passed into the prompt template, any constraints for what the output should not do, and any requirements that the output must adhere to.'\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  i want to do extraction over a page\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='Great! Could you please provide me with more details about the objective of the extraction? What specific information are you looking to extract from the page?'\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  i want the user to specify that at run time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content=\"Understood. So the objective of the prompt is to allow the user to specify the information they want to extract from a page at runtime. \\n\\nNow, let's move on to the variables. Are there any specific variables that you would like to pass into the prompt template? For example, the URL of the page or any other parameters that might be relevant for the extraction process.\"\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  the schema to extract, and the text to extract it from\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='Got it. So the variables that will be passed into the prompt template are the schema to extract and the text to extract it from.\\n\\nNext, are there any constraints for what the output should not do? For example, should the output not include any sensitive information or should it not exceed a certain length?'\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  it must be in json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='Understood. So a requirement for the output is that it must be in JSON format.\\n\\nLastly, are there any specific requirements that the output must adhere to? For example, should the output follow a specific structure or include certain fields?'\n",
      "\n",
      "---\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User (q/Q to quit):  must be json, must include the same fields as the schema specified\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from node 'info':\n",
      "---\n",
      "content='Got it. So the requirements for the output are that it must be in JSON format and it must include the same fields as the schema specified.\\n\\nBased on the information you provided, I will now generate the prompt template for extraction. Please give me a moment.\\n\\n' additional_kwargs={'tool_calls': [{'id': 'call_6roy9dQoIrQZsHffR9kjAr0e', 'function': {'arguments': '{\\n  \"objective\": \"Extract specific information from a page\",\\n  \"variables\": [\"schema\", \"text\"],\\n  \"constraints\": [\"Output should not include sensitive information\", \"Output should not exceed a certain length\"],\\n  \"requirements\": [\"Output must be in JSON format\", \"Output must include the same fields as the specified schema\"]\\n}', 'name': 'PromptInstructions'}, 'type': 'function'}]}\n",
      "\n",
      "---\n",
      "\n",
      "Output from node 'prompt':\n",
      "---\n",
      "content='Extract specific information from a page and output the result in JSON format. The input page should contain the following fields: {{schema}}. The extracted information should be stored in the variable {{text}}. Ensure that the output does not include any sensitive information and does not exceed a certain length. Additionally, the output should include the same fields as the specified schema.'\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    "while True:\n",
    "    user = input('User (q/Q to quit): ')\n",
    "    if user in {'q', 'Q'}:\n",
    "        print('AI: Byebye')\n",
    "        break\n",
    "    for output in graph.stream([HumanMessage(content=user)], config=config):\n",
    "        if \"__end__\" in output:\n",
    "            continue\n",
    "        # stream() yields dictionaries with output keyed by node name\n",
    "        for key, value in output.items():\n",
    "            print(f\"Output from node '{key}':\")\n",
    "            print(\"---\")\n",
    "            print(value)\n",
    "        print(\"\\n---\\n\")\n",
    "    history= output['__end__']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a276d20e-8a1b-4add-bf8d-83a8c803431d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
