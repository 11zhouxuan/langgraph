{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd80bc40-f10d-4ab3-826d-6cd0636d11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI, ChatAnthropic\n",
    "from langchain.prompts import SystemMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.runnables.openai_functions import OpenAIFunctionsRouter\n",
    "\n",
    "from permchain.connection_inmemory import InMemoryPubSubConnection\n",
    "from permchain.pubsub import PubSub\n",
    "from permchain.topic import Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c553be-9ed1-452c-a4ab-828f34dbb3ce",
   "metadata": {},
   "source": [
    "## Content Fetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6788d-b67c-4331-af8d-7741a66f03af",
   "metadata": {},
   "source": [
    "First, we are going to define our content fetcher. This is responsible for taking a search query an getting relevant web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9c32e92-6f19-4cf1-8b87-1b756de0e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "from langchain.document_loaders import AsyncHtmlLoader\n",
    "from langchain.document_transformers import Html2TextTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89d4a96a-2f59-491f-81fd-4a5d755c0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from duckduckgo_search import DDGS\n",
    "\n",
    "ddgs = DDGS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcdc3812-0cdc-4677-bf9d-f9d7d5cac7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query):\n",
    "    query = query.strip().strip('\"')\n",
    "    search_results = ddgs.text(query)\n",
    "    urls_to_look = []\n",
    "    for res in search_results:\n",
    "        if res.get(\"href\", None):\n",
    "            urls_to_look.append(res[\"href\"])\n",
    "        if len(urls_to_look) >= 4:\n",
    "            break\n",
    "\n",
    "    # Relevant urls\n",
    "    # Load, split, and add new urls to vectorstore\n",
    "    if urls_to_look:\n",
    "        loader = AsyncHtmlLoader(urls_to_look)\n",
    "        html2text = Html2TextTransformer()\n",
    "        docs = loader.load()\n",
    "        docs = list(html2text.transform_documents(docs))\n",
    "    else:\n",
    "        docs = []\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45bb7661-35db-4a67-a62e-9c791c9de359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d087d2b4-d1fa-4f63-81de-d3d4e1dc5b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = retrieve_documents(\"langchain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e0b79f-f6bd-4f68-9433-645ee1eea81e",
   "metadata": {},
   "source": [
    "## Summarizer\n",
    "We will now come up with an actor to summarize the results given a query and some search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "901e1f8d-c973-4998-a731-5dab0c147b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Answer the user's question given the search results\\n\\n<question>{question}</question><search_results>{search_results}</search_results>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "915bec33-d210-4471-b051-859ecba608be",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(max_retries=0).with_fallbacks(\n",
    "        [ChatOpenAI(model=\"gpt-3.5-turbo-16k\"), ChatAnthropic(model=\"claude-2\")]\n",
    "    )\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf403ec-39a4-4061-9401-06850ffe3761",
   "metadata": {},
   "source": [
    "## All together now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edc0def4-d184-4438-9099-e6604ba9ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer_inbox = Topic(\"summarizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23f89611-7b0f-4f01-b9a7-119124e3341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_actor = (\n",
    "    Topic.IN.subscribe()\n",
    "    | {\n",
    "        \"search_results\": retrieve_documents,\n",
    "        \"question\": Topic.IN.current(),\n",
    "    }\n",
    "    | summarizer_inbox.publish()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d4a6b51-bd93-47c2-a301-9593a47df7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_actor = (\n",
    "    summarizer_inbox.subscribe() | {\"answer\": summarizer_chain} | Topic.OUT.publish()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11d3b066-7f95-4ad9-82d0-ba64bbf3e3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_researcher = PubSub(\n",
    "    processes=(search_actor, summ_actor),\n",
    "    connection=InMemoryPubSubConnection(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc022d51-69f0-4da9-8025-70afdc3cc6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|#############################################################################################################################################################################################| 4/4 [00:00<00:00,  6.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'answer': 'LangSmith is a platform for building production-grade language model applications. It helps trace and evaluate language model applications and intelligent agents, making it easier to move from prototype to production. LangSmith is developed by LangChain, the company behind the open source LangChain framework. More information can be found in the LangSmith documentation.'}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_researcher.invoke(\"What is langsmith?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "000f4f24-15ba-476f-8a33-d023081b18d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages:   0%|                                                                                                                                                                                                     | 0/4 [00:00<?, ?it/s]\n",
      "Fetching pages: 100%|#############################################################################################################################################################################################| 4/4 [00:00<00:00,  6.93it/s]\u001b[A\n",
      "Fetching pages: 100%|#############################################################################################################################################################################################| 4/4 [00:00<00:00,  4.79it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +=: 'dict' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mweb_researcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is langsmith\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is llama\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/workplace/langchain/libs/langchain/langchain/schema/runnable/base.py:102\u001b[0m, in \u001b[0;36mRunnable.batch\u001b[0;34m(self, inputs, config, max_concurrency)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke(inputs[\u001b[38;5;241m0\u001b[39m], configs[\u001b[38;5;241m0\u001b[39m])]\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_concurrency) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/concurrent/futures/_base.py:608\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    606\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 608\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    610\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m fs\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mresult(end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/concurrent/futures/_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/workplace/permchain/permchain/pubsub.py:67\u001b[0m, in \u001b[0;36mPubSub.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     66\u001b[0m     collected \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;28minput\u001b[39m, config):\n\u001b[1;32m     68\u001b[0m         collected\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collected\n",
      "File \u001b[0;32m~/workplace/permchain/permchain/pubsub.py:176\u001b[0m, in \u001b[0;36mPubSub.stream\u001b[0;34m(self, input, config, max_concurrency)\u001b[0m\n\u001b[1;32m    174\u001b[0m             final_output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m             final_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# Cleanup\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m inflight:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +=: 'dict' and 'dict'"
     ]
    }
   ],
   "source": [
    "web_researcher.batch([\"what is langsmith\", \"what is llama\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952e9b8-2a56-4d2d-997a-dceb7652186f",
   "metadata": {},
   "source": [
    "## Trying to use it as a sub component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43ca019d-a500-4c77-8e62-a46e54ffae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers.openai_functions import JsonKeyOutputFunctionsParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "858a82ae-a73f-4da2-9210-298af789ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Write between 2 and 5 sub questions that serve as google search queries to search online that form an objective opinion from the following: {question}\"\"\"\n",
    "functions = [\n",
    "    {\n",
    "        \"name\": \"sub_questions\",\n",
    "        \"description\": \"List of sub questions\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"questions\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"description\": \"List of sub questions to ask.\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "question_chain = (\n",
    "    prompt\n",
    "    | ChatOpenAI(temperature=0).bind(\n",
    "        functions=functions, function_call={\"name\": \"sub_questions\"}\n",
    "    )\n",
    "    | JsonKeyOutputFunctionsParser(key_name=\"questions\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c0298fdc-0e7c-4e79-9cb7-cdd4d50fe88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What is the purpose of Langsmith?',\n",
       " 'Who developed Langsmith?',\n",
       " 'What are the features of Langsmith?',\n",
       " 'How does Langsmith work?',\n",
       " 'Are there any alternatives to Langsmith?']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_chain.invoke({\"question\": \"what is langsmith?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a0101bd0-cd95-4b13-ab26-d5d34d703bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are tasked with writing a research report to answer the following question:\n",
    "\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\n",
    "In order to do that, you first came up with several sub questions and researched those. please find those below:\n",
    "\n",
    "<research>\n",
    "{research}\n",
    "</research>\n",
    "\n",
    "Now, write your final report answering the original question!\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "report_chain = prompt | ChatOpenAI() | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7715e0c-23c0-4985-97a7-bf5b151cf734",
   "metadata": {},
   "outputs": [],
   "source": [
    "research_inbox = Topic(\"research\")\n",
    "writer_inbox = Topic(\"writer_inbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f274027a-c9f0-4efa-b798-1921b9b376d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "subquestion_actor = (\n",
    "    # Listed in inputs\n",
    "    Topic.IN.subscribe()\n",
    "    | question_chain\n",
    "    # The draft always goes to the editors inbox\n",
    "    | research_inbox.publish()\n",
    ")\n",
    "research_actor = (\n",
    "    research_inbox.subscribe()\n",
    "    | {\n",
    "        \"research\": lambda x: web_researcher.batch(x),\n",
    "        # \"research\": lambda x: [web_researcher.invoke({\"question\": i}) for i in x],\n",
    "        \"question\": Topic.IN.current() | itemgetter(\"question\"),\n",
    "    }\n",
    "    | writer_inbox.publish()\n",
    ")\n",
    "write_actor = writer_inbox.subscribe() | report_chain | Topic.OUT.publish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "789b636d-23ce-44fb-a8e3-fdfbfabe77ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "longer_researcher = PubSub(\n",
    "    processes=(subquestion_actor, research_actor, write_actor),\n",
    "    connection=InMemoryPubSubConnection(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f713a31-5f60-4d90-9b95-3f42d8fbbddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages:   0%|                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]\n",
      "Fetching pages:   0%|                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "Fetching pages:   0%|                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Fetching pages:   0%|                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Fetching pages:   0%|                                                                                                                                                                                               | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching pages: 100%|#######################################################################################################################################################################################| 4/4 [00:01<00:00,  3.84it/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fetching pages: 100%|#######################################################################################################################################################################################| 4/4 [00:01<00:00,  3.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Fetching pages: 100%|#######################################################################################################################################################################################| 4/4 [00:01<00:00,  2.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "Fetching pages: 100%|#######################################################################################################################################################################################| 4/4 [00:01<00:00,  2.85it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Fetching pages: 100%|#######################################################################################################################################################################################| 4/4 [00:01<00:00,  2.69it/s]\u001b[A\u001b[A\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo-16k in organization org-i0zjYONU3PemzJ222esBaAzZ on tokens per min. Limit: 180000 / min. Current: 173743 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo-16k in organization org-i0zjYONU3PemzJ222esBaAzZ on tokens per min. Limit: 180000 / min. Current: 161254 / min. Contact us through our help center at help.openai.com if you continue to have issues..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Research Report: Understanding LangSmith\\n\\nIntroduction:\\nThe purpose of this research report is to provide a comprehensive understanding of LangSmith, a developer platform designed to facilitate the development and management of Language Model applications (LLMs). Through an analysis of the gathered research, this report aims to answer the question: \"What is LangSmith?\"\\n\\nResearch Findings:\\n\\n1. LangSmith Overview:\\nLangSmith is a unified platform that helps developers trace, evaluate, and monitor LLM applications and intelligent agents. It aims to simplify the process of moving from prototype to production by providing features such as tracing runs, testing prompts or answers, and exporting datasets and runs for further analysis. LangSmith offers comprehensive visibility into the chain sequence of calls, real-time insights, and observability features to monitor LLM applications. It emphasizes best practices and offers useful tools and resources for developers working with LLMs.\\n\\n2. Key Features of LangSmith:\\nLangSmith offers several key features to assist developers in building and managing LLM applications. These include:\\n- Tracing and evaluating the behavior of LLM applications.\\n- Debugging and experimentation capabilities.\\n- Sharing work with others.\\n- Creating datasets for testing and evaluation.\\n- Evaluating models based on created datasets.\\n- Monitoring the behavior and performance of LLM applications.\\n- Comprehensive visibility into the entire chain sequence of calls.\\n\\n3. LangSmith\\'s Integration with LangChain:\\nLangSmith seamlessly integrates with LangChain, a library for prototyping LLM applications. This integration allows developers to leverage the composability of LangChain and build applications with large language models effectively. LangChain supports features such as memory, custom datasets, and more.\\n\\n4. Potential Alternatives to LangSmith:\\nBased on the research findings, several potential alternatives to LangSmith have been identified. These include:\\n- LangChain: An open-source framework for building applications with large language models through composability.\\n- GradientJ: A platform to build, orchestrate, and manage complex LLM applications at scale.\\n- LLMOps.Space: A community and resource hub focused on deploying LLMs into production.\\n- Vellum: A development platform aimed at production LLM applications, providing tools for monitoring, version control, and testing datasets.\\n- Llama2: An open-source large language model from Meta that can be fine-tuned and deployed.\\n- Openlayer: A platform focused on ML model testing, monitoring, and improvement.\\n- Backengine: A platform that allows creating and deploying backend APIs using natural language descriptions.\\n- QueryVary: A platform for systematically designing and refining prompts for LLMs.\\n\\nConclusion:\\nIn conclusion, LangSmith is a developer platform designed to simplify the development and management of Language Model applications (LLMs). It provides developers with tools for tracing, testing, evaluating, and monitoring LLM applications, along with comprehensive visibility and real-time insights. LangSmith aims to empower developers and handle the complexity of LLM applications effectively. By integrating seamlessly with LangChain, it offers enhanced capabilities for building applications with large language models. While LangSmith is a prominent platform, developers may also consider other alternatives such as LangChain, GradientJ, LLMOps.Space, Vellum, and more, depending on their specific requirements.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longer_researcher.invoke({\"question\": \"what is langsmith?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde8b227-64ec-4e96-b337-fc888e7ad787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
