{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3e3ebc4-57af-4fe4-bdd3-36aff67bf276",
   "metadata": {},
   "source": [
    "## Hierarchical Agent Teams\n",
    "\n",
    "In our previous example ([Agent Supervisor](./agent_supervisor.ipynb)), we introduced the concept of a single supervisor node to route work between different worker nodes.\n",
    "\n",
    "But what if the job for a single worker becomes too complex? What if the number of workers becomes too large?\n",
    "\n",
    "For some applications, the system may be more effective if work is distributed _hierarchically_.\n",
    "\n",
    "You can do this by composing different subgraphs and creating a top-level supervisor, along with mid-level supervisors.\n",
    "\n",
    "To do this, let's build a simple research assistant! The graph will look something like the following:\n",
    "\n",
    "![diagram](./img/hierarchical-diagram.png)\n",
    "\n",
    "This notebook is inspired by the paper [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155), by Wu, et. al. In the rest of this notebook, you will:\n",
    "\n",
    "1. Define some utilities to help create the graph and their relations\n",
    "2. Write the tools and agent implementations for each team\n",
    "3. Compose everything together.\n",
    "\n",
    "But before all of that, some setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d30b6f7-3bec-4d9f-af50-43dfdc81ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# %pip install -U langgraph langchain langchain_openai langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c2f3de-c730-4aec-85a6-af2c2f058803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "\n",
    "def _set_if_undefined(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass(f\"Please provide your {var}\")\n",
    "\n",
    "\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")\n",
    "_set_if_undefined(\"LANGCHAIN_API_KEY\")\n",
    "_set_if_undefined(\"TAVILY_API_KEY\")\n",
    "\n",
    "# Optional, add tracing in LangSmith.\n",
    "# This will help you visualize and debug the control flow\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Multi-agent Collaboration\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504ee1c6-2b6a-439d-9046-df54e1e15698",
   "metadata": {},
   "source": [
    "## Define Utilities\n",
    "\n",
    "We are going to create a few utility functions to make it more concise when we want to:\n",
    "\n",
    "1. Create a worker agent and add it to a graph.\n",
    "2. Create a supervisor for the sub-graph.\n",
    "\n",
    "These will simplify the graph compositional code at the end for us so it's easier to see what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09fb60f-1aac-455b-b67d-8d2e4ccfd747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, List, Optional, TypedDict, Union\n",
    "\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "\n",
    "def create_worker_agent(\n",
    "    graph_builder: StateGraph,\n",
    "    name: str,\n",
    "    llm: ChatOpenAI,\n",
    "    tools: list,\n",
    "    system_prompt: str,\n",
    "    prelude: Optional[Union[Runnable, Callable]] = None,  # Optional required steps\n",
    ") -> str:\n",
    "    \"\"\"Create a function-calling agent and add it to the graph.\"\"\"\n",
    "    system_prompt += \"\\nWork autonomously according to your specialty, using the tools available to you.\"\n",
    "    \" Do not ask for clarification.\"\n",
    "    \" Your other team members (and other teams) will collaborate with you with their own specialties.\"\n",
    "    \" You are chosen for a reason! You are one of the following team members: {team_members}.\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    chain = executor | (\n",
    "        lambda x: {\"messages\": [HumanMessage(content=x[\"output\"], name=name)]}\n",
    "    )\n",
    "    if prelude is not None:\n",
    "        chain = prelude | chain\n",
    "    graph_builder.add_node(name, chain)\n",
    "    return name\n",
    "\n",
    "\n",
    "def create_team_supervisor(\n",
    "    graph_builder: StateGraph, llm: ChatOpenAI, system_prompt: str\n",
    ") -> str:\n",
    "    \"\"\"An LLM-based router.\"\"\"\n",
    "    supervisor_id = uuid.uuid4().hex[:4]\n",
    "    supervisor_name = f\"supervisor - {supervisor_id}\"\n",
    "    members = list(graph_builder.nodes)\n",
    "    options = [\"FINISH\"] + members\n",
    "    function_def = {\n",
    "        \"name\": \"route\",\n",
    "        \"description\": \"Select the next role.\",\n",
    "        \"parameters\": {\n",
    "            \"title\": \"routeSchema\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"next\": {\n",
    "                    \"title\": \"Next\",\n",
    "                    \"anyOf\": [\n",
    "                        {\"enum\": options},\n",
    "                    ],\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"next\"],\n",
    "        },\n",
    "    }\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            (\n",
    "                \"system\",\n",
    "                \"Given the conversation above, who should act next?\"\n",
    "                \" Or should we FINISH? Select one of: {options}\",\n",
    "            ),\n",
    "        ]\n",
    "    ).partial(options=str(options), team_members=\", \".join(members))\n",
    "    chain = (\n",
    "        prompt\n",
    "        | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "        | JsonOutputFunctionsParser()\n",
    "    )\n",
    "    graph_builder.add_node(supervisor_name, chain)\n",
    "    conditional_map = {k: k for k in members}\n",
    "    conditional_map[\"FINISH\"] = END\n",
    "\n",
    "    for member in members:\n",
    "        graph_builder.add_edge(member, supervisor_name)\n",
    "    graph_builder.add_conditional_edges(\n",
    "        supervisor_name, lambda x: x[\"next\"], conditional_map\n",
    "    )\n",
    "    return supervisor_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00282b1f-bb4d-4ee7-9bae-e8e6f586f12e",
   "metadata": {},
   "source": [
    "## Define agents + tools\n",
    "\n",
    "Now we can get to define our hierachical teams. \"Choose your player!\"\n",
    "\n",
    "### Research Team\n",
    "\n",
    "The research team can use a search engine and url scraper to find information on the web. Feel free to add additional functionality below to boost the team performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f04c6778-403b-4b49-9b93-678e910d5cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "from langsmith import trace\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "\n",
    "@tool\n",
    "def scrape_webpages(urls: List[str]) -> str:\n",
    "    \"\"\"Use requests and bs4 to scrape the provided web pages for detailed information.\"\"\"\n",
    "    loader = WebBaseLoader(urls)\n",
    "    docs = loader.load()\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            f'<Document name=\"{doc.metadata[\"title\"]}\">\\n{doc.page_content}\\n</Document>'\n",
    "            for doc in docs\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53db0c78-e357-48ba-ae5f-3fc04735a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# Research team graph state\n",
    "class State(TypedDict):\n",
    "    # A message is added after each team member finishes\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    # The team members are tracked so they are aware of\n",
    "    # the others' skill-sets\n",
    "    team_members: List[str]\n",
    "    # Used to route work. The supervisor calls a function\n",
    "    # that will update this every time it makes a decision\n",
    "    next: str\n",
    "\n",
    "\n",
    "research_graph = StateGraph(State)\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "create_worker_agent(\n",
    "    research_graph,\n",
    "    \"Search\",\n",
    "    llm,\n",
    "    [tavily_tool],\n",
    "    \"You are a research assistant who can search for up-to-date info\"\n",
    "    \" using the tavily search engine.\",\n",
    ")\n",
    "create_worker_agent(\n",
    "    research_graph,\n",
    "    \"Web Scraper\",\n",
    "    llm,\n",
    "    [scrape_webpages],\n",
    "    \"You are a research assistant who can scrape\"\n",
    "    \" specified urls for more detailed information using\"\n",
    "    \" the scrape_webpages function.\",\n",
    ")\n",
    "supervisor_node = create_team_supervisor(\n",
    "    research_graph,\n",
    "    llm,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {team_members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\",\n",
    ")\n",
    "\n",
    "research_graph.set_entry_point(supervisor_node)\n",
    "\n",
    "\n",
    "# The following functions interoperate between the top level graph state\n",
    "# and the state of the research sub-graph\n",
    "# this makes it so that the states of each graph don't get intermixed\n",
    "def enter_chain(message: str, members: Optional[list] = None):\n",
    "    results = {\n",
    "        \"messages\": [HumanMessage(content=message)],\n",
    "    }\n",
    "    if members:\n",
    "        results[\"team_members\"] = \"\\n\".join(sorted(members))\n",
    "    return results\n",
    "\n",
    "\n",
    "def return_final_response(state):\n",
    "    return {\"final_response\": state[\"messages\"][-1]}\n",
    "\n",
    "\n",
    "research_chain = (\n",
    "    functools.partial(enter_chain, members=research_graph.nodes)\n",
    "    | research_graph.compile()\n",
    "    | return_final_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b99ab-f6f0-4c5d-a90b-10102465d186",
   "metadata": {},
   "source": [
    "## Document Writing Team\n",
    "\n",
    "We will construct a graph in a similar fashion. This time using different tools.\n",
    "\n",
    "Note that we are giving file-system access to our agent here, which is not safe in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "202806d6-80bf-4153-ac16-ed6059236f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tempfile import TemporaryDirectory\n",
    "from typing import Dict\n",
    "\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "_TEMP_DIRECTORY = TemporaryDirectory()\n",
    "WORKING_DIRECTORY = Path(_TEMP_DIRECTORY.name)\n",
    "\n",
    "\n",
    "@tool\n",
    "def create_outline(\n",
    "    points: Annotated[List[str], \"List of main points or sections.\"],\n",
    "    file_name: Annotated[str, \"File path to save the outline.\"],\n",
    ") -> Annotated[str, \"Path of the saved outline file.\"]:\n",
    "    \"\"\"Create and save an outline.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        for i, point in enumerate(points):\n",
    "            file.write(f\"{i + 1}. {point}\\n\")\n",
    "    return f\"Outline saved to {file_name}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def read_document(\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    "    start: Annotated[Optional[int], \"The start line. Default is 0\"] = None,\n",
    "    end: Annotated[Optional[int], \"The end line. Default is None\"] = None,\n",
    ") -> str:\n",
    "    \"\"\"Read the specified document.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    if start is not None:\n",
    "        start = 0\n",
    "    return \"\\n\".join(lines[start:end])\n",
    "\n",
    "\n",
    "@tool\n",
    "def write_document(\n",
    "    content: Annotated[str, \"Text content to be written into the document.\"],\n",
    "    file_name: Annotated[str, \"File path to save the document.\"],\n",
    ") -> Annotated[str, \"Path of the saved document file.\"]:\n",
    "    \"\"\"Create and save a text document.\"\"\"\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.write(content)\n",
    "    return f\"Document saved to {file_name}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def edit_document(\n",
    "    file_name: Annotated[str, \"Path of the document to be edited.\"],\n",
    "    inserts: Annotated[\n",
    "        Dict[int, str],\n",
    "        \"Dictionary where key is the line number (1-indexed) and value is the text to be inserted at that line.\",\n",
    "    ],\n",
    ") -> Annotated[str, \"Path of the edited document file.\"]:\n",
    "    \"\"\"Edit a document by inserting text at specific line numbers.\"\"\"\n",
    "    # Read the contents of the file\n",
    "\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Adjust the line numbers for 0-indexing and sort\n",
    "    sorted_inserts = sorted(inserts.items())\n",
    "\n",
    "    # Perform the insertions\n",
    "    for line_number, text in sorted_inserts:\n",
    "        if 1 <= line_number <= len(lines) + 1:\n",
    "            # Insert the text at the specified line number\n",
    "            lines.insert(line_number - 1, text + \"\\n\")\n",
    "        else:\n",
    "            return f\"Error: Line number {line_number} is out of range.\"\n",
    "\n",
    "    # Write the modified content back to the file\n",
    "    with (WORKING_DIRECTORY / file_name).open(\"w\") as file:\n",
    "        file.writelines(lines)\n",
    "\n",
    "    return f\"Document edited and saved to {file_name}\"\n",
    "\n",
    "\n",
    "# Warning: This executes code locally, which can be unsafe when not sandboxed\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "\n",
    "@tool\n",
    "def python_repl(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"]\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    return f\"Succesfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bcdbf44-9481-430c-8429-fa142ed8a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Document writing team graph state\n",
    "class AuthoringState(TypedDict):\n",
    "    # This tracks the team's conversation internally\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    # This provides each worker with context on the others' skill sets\n",
    "    team_members: str\n",
    "    # This is how the supervisor tells langgraph who to work next\n",
    "    next: str\n",
    "    # This tracks the shared directory state\n",
    "    current_files: str\n",
    "\n",
    "\n",
    "# This will be run before each worker agent begins work\n",
    "# It makes it so they are more aware of the current state\n",
    "# of the working directory.\n",
    "def prelude(state):\n",
    "    written_files = []\n",
    "    if not WORKING_DIRECTORY.exists():\n",
    "        WORKING_DIRECTORY.mkdir()\n",
    "    try:\n",
    "        written_files = [\n",
    "            f.relative_to(WORKING_DIRECTORY) for f in WORKING_DIRECTORY.rglob(\"*\")\n",
    "        ]\n",
    "    except:\n",
    "        pass\n",
    "    if not written_files:\n",
    "        return {**state, \"current_files\": \"No files written.\"}\n",
    "    return {\n",
    "        **state,\n",
    "        \"current_files\": \"\\nBelow are files your team has written to the directory:\\n\"\n",
    "        + \"\\n\".join([f\" - {f}\" for f in written_files]),\n",
    "    }\n",
    "\n",
    "\n",
    "# Create the graph here:\n",
    "authoring_graph = StateGraph(AuthoringState)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "create_worker_agent(\n",
    "    authoring_graph,\n",
    "    \"Author Docs\",\n",
    "    llm,\n",
    "    [write_document, edit_document, read_document],\n",
    "    \"You are an expert writing a research document.\\n\"\n",
    "    # The {current_files} value is populated automatically by the graph state\n",
    "    \"Below are files currently in your directory:\\n{current_files}\",\n",
    "    prelude=prelude,\n",
    ")\n",
    "create_worker_agent(\n",
    "    authoring_graph,\n",
    "    \"Outline + Notetaker\",\n",
    "    llm,\n",
    "    [create_outline, read_document],\n",
    "    \"You are an expert senior researcher tasked with writing a paper outline and\"\n",
    "    \" taking notes to craft a perfect paper.{current_files}\",\n",
    "    prelude=prelude,\n",
    ")\n",
    "create_worker_agent(\n",
    "    authoring_graph,\n",
    "    \"Generate Charts\",\n",
    "    llm,\n",
    "    [read_document, python_repl],\n",
    "    \"You are a data viz expert tasked with generating charts for a research project.\"\n",
    "    \"{current_files}\",\n",
    ")\n",
    "\n",
    "supervisor_node = create_team_supervisor(\n",
    "    authoring_graph,\n",
    "    llm,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {team_members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\",\n",
    ")\n",
    "\n",
    "authoring_graph.set_entry_point(supervisor_node)\n",
    "\n",
    "# We re-use the enter/exit functions to wrap the graph\n",
    "authoring_chain = (\n",
    "    functools.partial(enter_chain, members=authoring_graph.nodes)\n",
    "    | authoring_graph.compile()\n",
    "    | return_final_response\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b5b08d-9a9a-474a-94b4-f7aaa8ff19e6",
   "metadata": {},
   "source": [
    "## Add Layers\n",
    "\n",
    "In this design, we are enforcing a top-down planning policy. We've created two graphs already, but we have to decide how to route work between the two.\n",
    "\n",
    "We'll create a _third_ graph to orchestrate the previous two, and add some connectors to define how this top-level state is shared between the different graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95ae7e52-92ed-41a3-88c4-21b6d7c8b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# Research team graph\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    next: str\n",
    "\n",
    "\n",
    "def get_last_message(state: State) -> str:\n",
    "    return state[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "def join_graph(response: dict):\n",
    "    return {\"messages\": [response[\"final_response\"]]}\n",
    "\n",
    "\n",
    "super_graph = StateGraph(State)\n",
    "super_graph.add_node(\"Research team\", get_last_message | research_chain | join_graph)\n",
    "super_graph.add_node(\n",
    "    \"Paper writing team\", get_last_message | authoring_chain | join_graph\n",
    ")\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "supervisor_node = create_team_supervisor(\n",
    "    super_graph,\n",
    "    llm,\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following teams: {team_members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\",\n",
    ")\n",
    "\n",
    "super_graph.set_entry_point(supervisor_node)\n",
    "super_graph = enter_chain | super_graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b8badbf-d728-44bd-a2a7-5b4e587c92fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessage(content='The document titled \"North American Sturgeons: Overview\" has been successfully created and saved as `North_American_Sturgeons_Overview.txt`. This document includes key information on the distribution, habitat, and conservation status of various sturgeon species found in North America, as well as a simplified table summarizing these details.\\n\\nFor further reference or detailed information, you can access and read the document. If you require any additional information or updates to the document, please let me know.', name='Author Docs')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = super_graph.invoke(\n",
    "    \"Write a brief research report on the North American sturgeon. Include a chart.\",\n",
    "    {\"recursion_limit\": 150},\n",
    ")\n",
    "results[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ffcc7f-7b78-4ca5-8e0a-7c0ac08300fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
